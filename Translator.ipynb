{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datle1907/Translation-VietNam-to-English/blob/main/Translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TxkpnoR60tKC"
      },
      "outputs": [],
      "source": [
        "# In[1]: PART 1. IMPORT AND FUNCTIONS\n",
        "#region\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import joblib\n",
        "import gdown\n",
        "import zipfile\n",
        "from mosestokenizer import MosesTokenizer, MosesDetokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ys542Ym1_El-"
      },
      "outputs": [],
      "source": [
        "eos_id = 0 # end-of-seq token id\n",
        "sos_id = 1 # start-of-seq token id\n",
        "oov_id = 2 # out-of-vocab word id\n",
        "\n",
        "def word_to_id(word, vocab_list):\n",
        "    if word in vocab_list:\n",
        "        return vocab_list.index(word)\n",
        "    else:\n",
        "        return oov_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R98EPrxa0tMY"
      },
      "outputs": [],
      "source": [
        "def id_to_word(id, vocab_list):\n",
        "    return vocab_list[id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i3Hpok3B_Tqw"
      },
      "outputs": [],
      "source": [
        "# In[2]: PART 2. LOAD AND PREPROCESS DATA\n",
        "# Hyperparameters:\n",
        "N_WORDS_KEPT = 200 # Number of words to keep in each sample (a line in txt files)\n",
        "min_occurrences = 4 # Each word appear many times in the dataset. We only keep the words that occur > min_occurrences in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZYGyUMJ_TtI",
        "outputId": "6b36d6d7-d6ab-4ed2-9f55-5eacca4a6542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1AiUt7TuIUcVLb3M_iM99yGhJTtuhOB_x\n",
            "To: /content/temp.zip\n",
            "100%|██████████| 22.7M/22.7M [00:00<00:00, 35.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# LOAD DATA:\n",
        "# Orginal data source: https://opus.nlpl.eu/TED2020-v1.php\n",
        "unzip_to_path = r'datasets/TED2020_en-vi_txt'\n",
        "source_language_data_path = r'datasets/TED2020_en-vi_txt/TED2020.en-vi.vi'\n",
        "target_language_data_path = r'datasets/TED2020_en-vi_txt/TED2020.en-vi.en'\n",
        "\n",
        "new_download = True\n",
        "if new_download:\n",
        "    url_data = 'https://drive.google.com/u/0/uc?id=1AiUt7TuIUcVLb3M_iM99yGhJTtuhOB_x'\n",
        "    download_output = 'temp.zip'\n",
        "    gdown.download(url_data, download_output, quiet=False)\n",
        "    with zipfile.ZipFile(download_output, 'r') as zip_f:\n",
        "        zip_f.extractall(unzip_to_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVELNHb6_Tvy",
        "outputId": "1f4d1ccb-de3e-49cd-909b-a10929d2ce5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VI samples:\n",
            "Cám ơn rất nhiều, Chris. \n",
            "\n",
            "Đây thật sự là một vinh hạnh lớn cho tôi khi có cơ hội được đứng trên sân khấu này hai lần; Tôi thật sự rất cảm kích. \n",
            "\n",
            "Tôi thực sự bị choáng ngợp bởi hội nghị này, và tôi muốn cám ơn tất cả các bạn vì rất nhiều nhận xét tốt đẹp về những gì tôi đã trình bày đêm hôm trước. \n",
            "\n",
            "vi_list length: 326417\n"
          ]
        }
      ],
      "source": [
        "f = open(source_language_data_path, 'r', encoding='utf-8')\n",
        "vi_list = f.readlines()\n",
        "f.close()\n",
        "print('VI samples:')\n",
        "[print(sentence) for sentence in vi_list[:3]]\n",
        "print('vi_list length:', len(vi_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8CYLiXJ_Txx",
        "outputId": "44e4d60f-db6d-49cc-fef7-50b4c4fb61b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EN samples:\n",
            "Thank you so much, Chris. \n",
            "\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful. \n",
            "\n",
            "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night. \n",
            "\n",
            "en_list length: 326417\n"
          ]
        }
      ],
      "source": [
        "f = open(target_language_data_path, 'r', encoding='utf-8')\n",
        "en_list = f.readlines()\n",
        "f.close()\n",
        "print('\\n\\nEN samples:')\n",
        "[print(sentence) for sentence in en_list[:3]]\n",
        "print('en_list length:', len(en_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HUeqlH4s_T0C"
      },
      "outputs": [],
      "source": [
        "# PREPROCESS DATA:\n",
        "# Delete all \\n:\n",
        "vi_list = [i.replace('\\n', ' ') for i in vi_list]\n",
        "en_list = [i.replace('\\n', ' ') for i in en_list]\n",
        "\n",
        "# Add spaces around digits: (otherwise a lot of numbers, e.g., 102, 103, 25648, are in vocab)\n",
        "marks = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "for mark in marks:\n",
        "    vi_list = [i.replace(mark, ' ' + mark + ' ') for i in vi_list]\n",
        "    en_list = [i.replace(mark, ' ' + mark + ' ') for i in en_list]\n",
        "\n",
        "# Add spaces around punctuation to help MosesTokenizer not to keep these 'words'\n",
        "marks = ['.', ',', ':', '!', '?', '-', '_']\n",
        "for mark in marks:\n",
        "    vi_list = [i.replace(mark, ' ' + mark + ' ') for i in vi_list]\n",
        "    en_list = [i.replace(mark, ' ' + mark + ' ') for i in en_list]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OojlKxzT_n9y",
        "outputId": "e1a0c1fa-1bd7-44e3-ca03-2ef38d9724b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done making word lists.\n"
          ]
        }
      ],
      "source": [
        "load_word_lists = False\n",
        "if not load_word_lists:\n",
        "    from mosestokenizer import MosesTokenizer, MosesDetokenizer\n",
        "    en_tokenize = MosesTokenizer('en')\n",
        "    vi_tokenize = MosesTokenizer('vi')\n",
        "    en_list_tokenized = []\n",
        "    vi_list_tokenized = []\n",
        "    vi_list_filtered = []\n",
        "    en_list_filtered = []\n",
        "    for vi_i, en_i in zip(vi_list, en_list):\n",
        "        en_tokens = en_tokenize(en_i)\n",
        "        vi_tokens = vi_tokenize(vi_i)\n",
        "        if en_tokens!=[] and vi_tokens!=[]: # since some sentences become empty after tokenization\n",
        "            #!! Truncate sentences !!\n",
        "            en_list_tokenized.append(en_tokens[:N_WORDS_KEPT])\n",
        "            vi_list_tokenized.append(vi_tokens[:N_WORDS_KEPT])\n",
        "            vi_list_filtered.append(vi_i)\n",
        "            en_list_filtered.append(en_i)\n",
        "    en_tokenize.close()\n",
        "    vi_tokenize.close()\n",
        "\n",
        "    n_samples = len(vi_list_filtered)\n",
        "    joblib.dump(n_samples, r'./datasets/n_samples.joblib')\n",
        "    joblib.dump(en_list_tokenized, r'./datasets/en_list_tokenized.joblib')\n",
        "    joblib.dump(vi_list_tokenized, r'./datasets/vi_list_tokenized.joblib')\n",
        "    joblib.dump(en_list_filtered, r'./datasets/en_list_filtered.joblib')\n",
        "    joblib.dump(vi_list_filtered, r'./datasets/vi_list_filtered.joblib')\n",
        "    print('Done making word lists.')\n",
        "else:\n",
        "    n_samples = joblib.load(r'./datasets/n_samples.joblib')\n",
        "    en_list_tokenized = joblib.load(r'./datasets/en_list_tokenized.joblib')\n",
        "    vi_list_tokenized = joblib.load(r'./datasets/vi_list_tokenized.joblib')\n",
        "    en_list_filtered = joblib.load(r'./datasets/en_list_filtered.joblib')\n",
        "    vi_list_filtered = joblib.load(r'./datasets/vi_list_filtered.joblib')\n",
        "    print('Done loading word lists.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy8WBuFS_r1o",
        "outputId": "f4ca009a-cce5-4f15-8957-e6ae2aae5c5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(34905,)\n",
            "(70718,)\n",
            "27169\n",
            "11018\n",
            "\n",
            " [[1257, 322, 62, 60, 3, 1604, 4], [187, 131, 28, 5, 9, 1675, 789, 118, 26, 6, 40, 8, 128, 206, 20, 593, 70, 943, 1481, 21, 200, 225, 508, 43, 131, 28, 62, 126, 677, 4]]\n",
            "\n",
            " [['Cám', 'ơn', 'rất', 'nhiều', ',', 'Chris', '.'], ['Đây', 'thật', 'sự', 'là', 'một', 'vinh', 'hạnh', 'lớn', 'cho', 'tôi', 'khi', 'có', 'cơ', 'hội', 'được', 'đứng', 'trên', 'sân', 'khấu', 'này', 'hai', 'lần', ';', 'Tôi', 'thật', 'sự', 'rất', 'cảm', 'kích', '.']]\n",
            "Done encoding data.\n",
            "Done saving.\n",
            "\n",
            "DONE loading and preprocessing data.\n"
          ]
        }
      ],
      "source": [
        "load_processed_data = False\n",
        "if not load_processed_data:\n",
        "    # Create vocabularies:\n",
        "    vi_words = [words for sentence in vi_list_tokenized for words in sentence]\n",
        "    vi_vocab, vi_counts = np.unique(vi_words, return_counts=True)\n",
        "    vi_vocab_count = {word:count for word, count in zip(vi_vocab, vi_counts)}\n",
        "    print(vi_vocab.shape)\n",
        "\n",
        "    en_words = [words for sentence in en_list_tokenized for words in sentence]\n",
        "    en_vocab, en_counts = np.unique(en_words, return_counts=True)\n",
        "    en_vocab_count = {word:count for word, count in zip(en_vocab, en_counts)}\n",
        "    print(en_vocab.shape)\n",
        "\n",
        "    # Truncate the vocabulary (keep only words that appear at least min_occurrences times)\n",
        "    truncated_en_vocab = dict(filter(lambda ele: ele[1]>min_occurrences,en_vocab_count.items()))\n",
        "    truncated_en_vocab = dict(sorted(truncated_en_vocab.items(), key=lambda item: item[1], reverse=True)) # Just to have low ids for most appeared words\n",
        "    en_vocab_size = len(truncated_en_vocab)\n",
        "    print(en_vocab_size)\n",
        "    joblib.dump(en_vocab_size,r'./datasets/en_vocab_size.joblib')\n",
        "\n",
        "    truncated_vi_vocab = dict(filter(lambda ele: ele[1]>min_occurrences,vi_vocab_count.items()))\n",
        "    truncated_vi_vocab = dict(sorted(truncated_vi_vocab.items(), key=lambda item: item[1], reverse=True)) # Just to have low ids for most appeared words\n",
        "    vi_vocab_size = len(truncated_vi_vocab)\n",
        "    print(vi_vocab_size)\n",
        "    joblib.dump(vi_vocab_size,r'./datasets/vi_vocab_size.joblib')\n",
        "\n",
        "    # Convert words to ids:\n",
        "    vi_vocab_list = ['<eos>', '<sos>', '<oov>']\n",
        "    vi_vocab_list.extend(list(truncated_vi_vocab.keys()))\n",
        "    joblib.dump(vi_vocab_list,r'./datasets/vi_vocab_list.joblib')\n",
        "    en_vocab_list = ['<eos>', '<sos>', '<oov>']\n",
        "    en_vocab_list.extend(list(truncated_en_vocab.keys()))\n",
        "    joblib.dump(en_vocab_list,r'./datasets/en_vocab_list.joblib')\n",
        "\n",
        "    # Try encode, decoding some samples:\n",
        "    temp_vi_encode = [list(map(lambda word: word_to_id(word, vi_vocab_list), sentence)) for sentence in vi_list_tokenized[:2]]\n",
        "    print('\\n',temp_vi_encode)\n",
        "    temp_vi_decode = [list(map(lambda id: id_to_word(id, vi_vocab_list), sentence)) for sentence in temp_vi_encode]\n",
        "    print('\\n',temp_vi_decode)\n",
        "\n",
        "    # Convert the whole dataset:\n",
        "    #   X_vi_data: list of lists of token ids of vi_list_tokenized\n",
        "    #   Y_en_data: list of lists of token ids for en_list_tokenized\n",
        "    X_vi_data = [list(map(lambda word: word_to_id(word, vi_vocab_list), sentence)) for sentence in vi_list_tokenized]\n",
        "    Y_en_data = [list(map(lambda word: word_to_id(word, en_vocab_list), sentence)) for sentence in en_list_tokenized]\n",
        "\n",
        "    # Add end-of-seq and start-of-seq tokens:\n",
        "    X_vi_data =[[sos_id]+sentence+[eos_id] for sentence in X_vi_data]\n",
        "    Y_en_data =[[sos_id]+sentence+[eos_id] for sentence in Y_en_data]\n",
        "    Y_seq_lens = [len(sentence) for sentence in Y_en_data]\n",
        "\n",
        "    # Pad zero to have all sentences of the same length (required when converting to np.array):\n",
        "    max_X_len = np.max([len(sentence) for sentence in X_vi_data])\n",
        "    max_Y_len = np.max([len(sentence) for sentence in Y_en_data])\n",
        "    X_padded = [sentence + [0]*(max_X_len - len(sentence)) for sentence in X_vi_data]\n",
        "    Y_padded = [sentence + [0]*(max_Y_len - len(sentence)) for sentence in Y_en_data]\n",
        "    print('Done encoding data.')\n",
        "\n",
        "    joblib.dump(X_padded, r'./datasets/X_padded.joblib')\n",
        "    joblib.dump(Y_padded, r'./datasets/Y_padded.joblib')\n",
        "    joblib.dump(Y_seq_lens, r'./datasets/Y_seq_lens.joblib')\n",
        "    print('Done saving.')\n",
        "else:\n",
        "    Y_seq_lens = joblib.load(r'./datasets/Y_seq_lens.joblib')\n",
        "    vi_vocab_size = joblib.load(r'./datasets/vi_vocab_size.joblib')\n",
        "    en_vocab_size = joblib.load(r'./datasets/en_vocab_size.joblib')\n",
        "    X_padded = joblib.load(r'./datasets/X_padded.joblib')\n",
        "    Y_padded = joblib.load(r'./datasets/Y_padded.joblib')\n",
        "    print('Done loading converted data.')\n",
        "\n",
        "vocab_X_size = vi_vocab_size + 3\n",
        "vocab_Y_size = en_vocab_size + 3\n",
        "print('\\nDONE loading and preprocessing data.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fXYFBnbVYrI"
      },
      "outputs": [],
      "source": [
        "# TRAIN AN ENCODER–DECODER MODEL\n",
        "#region\n",
        "# Hyperparameters:\n",
        "n_units = 256 # NOTE: HYPERPARAM.\n",
        "embed_X_size = 30 # NOTE: HYPERPARAM. Size of embedding output for encoder.\n",
        "embed_Y_size = 50 # NOTE: HYPERPARAM. Size of embedding output for decoder.\n",
        "n_epochs = 100 # NOTE: HYPERPARAM. Number of epochs to run training.\n",
        "batch_size = 64 # NOTE: HYPERPARAM. batch_size\n",
        "\n",
        "# Encoder (4 GRU layers):\n",
        "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32, name='encoder_inputs')\n",
        "encoder_embedder = keras.layers.Embedding(vocab_X_size, embed_X_size, mask_zero=True) # set mask_zero=True may cause no GPU training\n",
        "encoder_embeddings = encoder_embedder(encoder_inputs)\n",
        "temp_output = keras.layers.GRU(units=n_units, return_sequences=True)(encoder_embeddings)\n",
        "temp_output = keras.layers.GRU(units=n_units, return_sequences=True)(temp_output)\n",
        "temp_output = keras.layers.GRU(units=n_units, return_sequences=True)(temp_output)\n",
        "encoder_output = keras.layers.GRU(units=n_units, return_state=True, name='encoder_final_layer')(temp_output)\n",
        "encoder_outputs, encoder_state = encoder_output\n",
        "\n",
        "# Decoder (4 GRU layers):\n",
        "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "decoder_embedder = keras.layers.Embedding(input_dim=vocab_Y_size, output_dim=embed_Y_size, name='decoder_embedder', mask_zero=True)\n",
        "decoder_embeddings = decoder_embedder(decoder_inputs)\n",
        "temp_output = keras.layers.GRU(units=n_units, return_sequences=True)(decoder_embeddings, initial_state=encoder_state)\n",
        "temp_output = keras.layers.GRU(units=n_units, return_sequences=True)(temp_output, initial_state=encoder_state)\n",
        "temp_output = keras.layers.GRU(units=n_units, return_sequences=True)(temp_output, initial_state=encoder_state)\n",
        "RNN_output = keras.layers.GRU(units=n_units, return_sequences=True)(temp_output, initial_state=encoder_state)\n",
        "output_layer = keras.layers.Dense(vocab_Y_size, activation='softmax', name='output_layer')\n",
        "decoder_output = output_layer(RNN_output)\n",
        "\n",
        "# The model:\n",
        "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[decoder_output])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='nadam', metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "#%% Train model:\n",
        "X = np.array(X_padded)\n",
        "Y = np.array(Y_padded)\n",
        "X_for_decoder = np.c_[np.zeros((n_samples, 1)), Y[:, :-1]] # X_for_decoder = [<pad> Y_t-1]. 0: <padding> or <eos>\n",
        "\n",
        "new_training = True\n",
        "if new_training:\n",
        "    checkpoint_name = r'models/encoder_decoder_multilayerGRU'+'_epoch{epoch:02d}_accuracy{accuracy:.4f}'\n",
        "    model_checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_name, monitor='accuracy',save_best_only=True)\n",
        "    early_stop = keras.callbacks.EarlyStopping(monitor='accuracy',patience=10,restore_best_weights=True)\n",
        "    tensorboard = keras.callbacks.TensorBoard(r'logs/translator_train_log',embeddings_freq=1, embeddings_metadata='embed_file')\n",
        "    model.fit([X, X_for_decoder], Y, epochs=n_epochs, batch_size=batch_size,\n",
        "        callbacks = [model_checkpoint, early_stop, tensorboard] )\n",
        "    #model.save(r'models/encoder_decoder_multilayerGRU')\n",
        "    print('DONE training.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UB47sVI_tEP",
        "outputId": "5eba624f-629d-47a5-cf89-4ef8e6f96a3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
          ]
        }
      ],
      "source": [
        "# LOAD MODEL\n",
        "### Load model\n",
        "zip_path = '/content/encoder_decoder_multilayerGRU.zip'\n",
        "\n",
        "# Directory to extract the contents\n",
        "extract_dir = '/content/extracted_model'\n",
        "\n",
        "# Unzip the model file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "\n",
        "# Load the extracted model\n",
        "model = keras.models.load_model(extract_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "A9I9mxjN_tG0"
      },
      "outputs": [],
      "source": [
        "# MAKE PREDICTIONS\n",
        "\n",
        "def predict_output(X_batch, max_output_length=50):\n",
        "    n_samples, n_steps = X_batch.shape\n",
        "    Y_pred = np.ones((n_samples, 1), dtype=np.int32) # y_t=0 = <pad> token id\n",
        "    for i in range(max_output_length):\n",
        "        Y_probas_next = model.predict([X_batch, Y_pred])[:, i:i+1]\n",
        "        Y_pred_next = np.argmax(Y_probas_next, axis=-1).astype(dtype=np.int32)\n",
        "        Y_pred = np.concatenate([Y_pred, Y_pred_next], axis=1)\n",
        "    # Only retain one <eos> token:\n",
        "    Y_pred_list = []\n",
        "    for i in range(len(Y_pred)):\n",
        "        Y_pred_list.append(Y_pred[i])\n",
        "        Y_pred_list[i] = list(np.trim_zeros(Y_pred_list[i], trim='b'))\n",
        "        Y_pred_list[i].append(0)\n",
        "    return Y_pred_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4Sd7njf-_6m1"
      },
      "outputs": [],
      "source": [
        "# DECODE PREDICTIONS\n",
        "def decode_Y_pred(Y_pred, en_vocab_list):\n",
        "    from mosestokenizer import MosesTokenizer, MosesDetokenizer\n",
        "    word_seq_list = [list(map(lambda id: id_to_word(id, en_vocab_list), id_seq)) for id_seq in Y_pred]\n",
        "    word_seq_no_se_tokens = []\n",
        "    for seq in word_seq_list: # Remove <sos> and <eos> tokens\n",
        "        no_se_seq = [i for i in seq if i != '<sos>' and i != '<eos>']\n",
        "        word_seq_no_se_tokens.append(no_se_seq)\n",
        "    detokenize = MosesDetokenizer('en')\n",
        "    decode_seq = [detokenize(seq) for seq in word_seq_no_se_tokens]\n",
        "    detokenize.close()\n",
        "    return decode_seq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data(vi_list, vi_vocab_list):\n",
        "    # Delete all \\n:\n",
        "    vi_list = [i.replace('\\n',' ') for i in vi_list]\n",
        "\n",
        "    # Add spaces around digits:\n",
        "    marks = ['0','1','2','3','4','5','6','7','8','9']\n",
        "    for mark in marks:\n",
        "        vi_list = [i.replace(mark,' '+mark+' ') for i in vi_list]\n",
        "\n",
        "    # Add spaces around punctuation:\n",
        "    marks = ['.', ',', ':', '!', '?', '-', '_']\n",
        "    for mark in marks:\n",
        "        vi_list = [i.replace(mark,' '+mark+' ') for i in vi_list]\n",
        "\n",
        "    # Tokenize text using Moses tokenizer:\n",
        "    from mosestokenizer import MosesTokenizer, MosesDetokenizer\n",
        "    vi_tokenize = MosesTokenizer('vi')\n",
        "    vi_list_tokenized = [vi_tokenize(vi_i) for vi_i in vi_list]\n",
        "    vi_tokenize.close()\n",
        "\n",
        "    # Convert words to ids:\n",
        "    X_vi_data = [list(map(lambda word: word_to_id(word, vi_vocab_list), sentence)) for sentence in vi_list_tokenized]\n",
        "\n",
        "    # Add end-of-seq and start-of-seq tokens:\n",
        "    X_vi_data = [[sos_id] + sentence + [eos_id] for sentence in X_vi_data]\n",
        "\n",
        "    # Padding to make the lengths of sentences equal:\n",
        "    max_X_len = np.max([len(sentence) for sentence in X_vi_data])\n",
        "    X_padded = [sentence + [0] * (max_X_len - len(sentence)) for sentence in X_vi_data]\n",
        "    print('Done encoding data.')\n",
        "\n",
        "    X_processed = np.array(X_padded)\n",
        "    return X_processed"
      ],
      "metadata": {
        "id": "fPTGuvBn2-mU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6xxqAUD_-wm",
        "outputId": "c9db92c2-62a5-49ec-e0e8-b6064f493f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Input strings (Vietnamese):\n",
            "\n",
            "     Thật may mắn khi làm việc với bạn.\n",
            "\n",
            "     Cảm ơn bạn nhiều nhé\n",
            "Done encoding data.\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "\n",
            "\n",
            "Translation (English):\n",
            "\n",
            "     It's lucky to work with you.\n",
            "\n",
            "     Thank you very much.\n"
          ]
        }
      ],
      "source": [
        "# In[7]: PART 7. DEMONSTRATION WITH TEST STRINGS\n",
        "#region\n",
        "# Vietnamese input strings to be translated:\n",
        "vi_raw_strings = [\"Thật may mắn khi làm việc với bạn.\", 'Cảm ơn bạn nhiều nhé']\n",
        "print('\\n\\nInput strings (Vietnamese):')\n",
        "for seq in vi_raw_strings:\n",
        "    print('\\n    ', seq)\n",
        "\n",
        "# Load vocabularies\n",
        "vi_vocab_list = joblib.load(r'datasets/vi_vocab_list.joblib')\n",
        "en_vocab_list = joblib.load(r'datasets/en_vocab_list.joblib')\n",
        "\n",
        "# Process the input data\n",
        "X_test = process_data(vi_raw_strings, vi_vocab_list)\n",
        "\n",
        "# Make predictions\n",
        "Y_pred = predict_output(X_test, max_output_length=25)\n",
        "\n",
        "# Decode predictions\n",
        "print('\\n\\nTranslation (English):')\n",
        "for seq in decode_Y_pred(Y_pred, en_vocab_list):\n",
        "    print('\\n    ', seq)\n",
        "#endregion"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuKY8UNcnRCbr++73q6+Va",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}